{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3e26f04-80ba-4b62-a389-2ceca6674fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breast Cancer Prediction \n",
    "# Using Neural Networks and Tensorflow\n",
    "# Written by: Prakash R. Kota\n",
    "# Written on: 06 Feb 2025\n",
    "# Last update: 11 Feb 2025\n",
    "\n",
    "# Data Set from\n",
    "# Original: \n",
    "#     https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic\n",
    "# With Header:\n",
    "#     https://www.kaggle.com/code/nancyalaswad90/analysis-breast-cancer-prediction-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a0dbd04-30cc-4188-8256-64aadf517f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU\n",
    "from tensorflow.keras.activations import linear, relu, sigmoid\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61a37d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the raw data and set up the \n",
    "# Training set and Test set\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file (ensure the header is recognized)\n",
    "df = pd.read_csv(\"./data/breastcancer.csv\")\n",
    "\n",
    "# Extract headers (column names)\n",
    "headers = df.columns.tolist()  # Stores header names in a list\n",
    "\n",
    "# Extract X (all columns except the columns 0 and 1)\n",
    "X = df.iloc[:, 2:].values  # Converts to NumPy array\n",
    "\n",
    "# Extract Y (column 1 only)\n",
    "Y_tmp = df.iloc[:, 1].values   # Converts to NumPy array\n",
    "# Replace 'M' with 1.0 and 'B' with 0.0\n",
    "Y = np.where(Y_tmp == \"M\", 1.0, 0.0).astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d400a3dc-0c77-4462-8415-323800e23d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X has 569 Training sets\n",
    "# Select a subset from rows 27 to 569,\n",
    "# that is index rows 26 to 569\n",
    "# Make this the training set\n",
    "X_train = X[26:570, :]\n",
    "y_train = Y[26:570]\n",
    "\n",
    "# Also create the Test set from the first 25 rows\n",
    "X_test = X[0:26, :]\n",
    "y_test = Y[0:26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e29c9b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCALING of data is very important\n",
    "# for convergence of the algorithm and for a good fit\n",
    "\n",
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on Training data and also transform\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "# Scale the Test data using X_train parameters\n",
    "X_test_scaled = scaler.transform(X_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0fea150-6b1f-4cf1-8e3c-31cc23189f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Neural Network\n",
    "# Layer0 - 30 Inputs\n",
    "# Layer1 - 25 Neurons and has 775 parameters\n",
    "# Layer2 - 15 Neurons and has 390 parameters\n",
    "# Layer3 - 1 Neuron (output) and has 16 parameters\n",
    "\n",
    "# Step1 - Define the Model\n",
    "model = Sequential(\n",
    "    [ \n",
    "        tf.keras.Input(shape=(30,)),\n",
    "        Dense(25, activation = 'relu', name='layer1'),\n",
    "        Dense(15, activation = 'relu', name='layer2'),\n",
    "        Dense(1, activation = 'sigmoid', name='layer3')    # < sigmoid activation here\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75f123c4-8a58-4340-95a0-a0c456cc08bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the model\n",
    "# For this define the Shape of input in the above model as follows\n",
    "# tf.keras.Input(shape=(30,)),\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35257f08-25f9-4c8b-a190-bb917d4e695b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step2 - Compile the model\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b23f3cc-5948-42d1-801b-72051d9f1c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6992\n",
      "Epoch 2/40\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5334 \n",
      "Epoch 3/40\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4200 \n",
      "Epoch 4/40\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3354 \n",
      "Epoch 5/40\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2725 \n",
      "Epoch 6/40\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2031 \n",
      "Epoch 7/40\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1752 \n",
      "Epoch 8/40\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1321 \n",
      "Epoch 9/40\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1287 \n",
      "Epoch 10/40\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1135 \n",
      "Epoch 11/40\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1108 \n",
      "Epoch 12/40\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0748 \n",
      "Epoch 13/40\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0977 \n",
      "Epoch 14/40\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0706 \n",
      "Epoch 15/40\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0691 \n",
      "Epoch 16/40\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0773 \n",
      "Epoch 17/40\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0618 \n",
      "Epoch 18/40\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0606 \n",
      "Epoch 19/40\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0426 \n",
      "Epoch 20/40\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0655 \n",
      "Epoch 21/40\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0394 \n",
      "Epoch 22/40\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0642 \n",
      "Epoch 23/40\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0369 \n",
      "Epoch 24/40\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0586 \n",
      "Epoch 25/40\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0483 \n",
      "Epoch 26/40\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0351 \n",
      "Epoch 27/40\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0424 \n",
      "Epoch 28/40\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0307 \n",
      "Epoch 29/40\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0330 \n",
      "Epoch 30/40\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0406 \n",
      "Epoch 31/40\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0365 \n",
      "Epoch 32/40\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0396 \n",
      "Epoch 33/40\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0485 \n",
      "Epoch 34/40\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0266 \n",
      "Epoch 35/40\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0283 \n",
      "Epoch 36/40\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0262 \n",
      "Epoch 37/40\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0223 \n",
      "Epoch 38/40\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0303 \n",
      "Epoch 39/40\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0330 \n",
      "Epoch 40/40\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0374 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x13ea338c0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step3 - Fit the model with the Training set\n",
    "model.fit(\n",
    "    X_train_scaled,y_train,\n",
    "    epochs=40\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f435d341-f831-49df-b090-fed07edd1a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print the Prediction for the\n",
      "first 25 rows of the Training set\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0]\n",
      "Print the Actual known outputs for the\n",
      "first 25 rows of the Training set\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0]\n",
      "Training set Prediction Accuracy: 99.26%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make Predictions with the known Training Set\n",
    "\n",
    "# From the above Tensorflow Model\n",
    "# Predict the first 25 outputs based on the Training set inputs\n",
    "print(f\"\"\"Print the Prediction for the\n",
    "first 25 rows of the Training set\"\"\")\n",
    "y_pred_train_raw = model.predict(X_train_scaled)\n",
    "# Convert probabilities to binary class labels (0 or 1)\n",
    "y_pred_train = (y_pred_train_raw > 0.5).astype(int)\n",
    "y_pred_train_list = y_pred_train.flatten().tolist()\n",
    "print(y_pred_train_list[:25])\n",
    "\n",
    "# Compare the Actual output values from the Training set\n",
    "print(f\"\"\"Print the Actual known outputs for the\n",
    "first 25 rows of the Training set\"\"\")\n",
    "y_train_list = y_train.astype(int).tolist()\n",
    "print(y_train_list[:25])\n",
    "\n",
    "# Find the Accuracy - Real vs Prediction, if real is known\n",
    "# as in the case of the Training set\n",
    "# Compute accuracy\n",
    "accuracy_train = np.mean(y_pred_train.flatten() == y_train.flatten()) # Flatten to match shapes\n",
    " \n",
    "print(f\"Training set Prediction Accuracy: {accuracy_train*100:.2f}%\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "77905f86-c420-499d-bfe6-630d4fe97250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print the Prediction for all the\n",
      "26 rows of the Test set\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1]\n",
      "Print the Actual known outputs for the\n",
      "26 rows of the Test set\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1]\n",
      "Test set Prediction Accuracy: 100.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make Predictions with the known Test Set\n",
    "\n",
    "# From the above Tensorflow Model\n",
    "# Predict the outputs based on the Test set inputs\n",
    "print(f\"\"\"Print the Prediction for all the\n",
    "26 rows of the Test set\"\"\")\n",
    "y_pred_test_raw = model.predict(X_test_scaled)\n",
    "# Convert probabilities to binary class labels (0 or 1)\n",
    "y_pred_test = (y_pred_test_raw > 0.5).astype(int)\n",
    "y_pred_test_list = y_pred_test.flatten().tolist()\n",
    "print(y_pred_test_list)\n",
    "\n",
    "# Compare the Actual output values from the Test set\n",
    "print(f\"\"\"Print the Actual known outputs for the\n",
    "26 rows of the Test set\"\"\")\n",
    "y_test_list = y_test.astype(int).tolist()\n",
    "print(y_test_list)\n",
    "\n",
    "\n",
    "# Find the Accuracy - Real vs Prediction, if real is known\n",
    "# as in the case of the Test set\n",
    "# Compute accuracy\n",
    "accuracy_test = np.mean(y_pred_test.flatten() == y_test.flatten()) # Flatten to match shapes\n",
    " \n",
    "print(f\"Test set Prediction Accuracy: {accuracy_test*100:.2f}%\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b70b3b-ff24-4418-9dfe-a582b4c075cf",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
